# Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, roc_auc_score
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Step 1: Load the dataset
df = pd.read_csv("data/ehr_data.csv")  # Replace with your dataset path

# Step 2: Data Cleaning
# Remove columns with >30% missing data
missing_threshold = 0.3
df = df.loc[:, df.isnull().mean() < missing_threshold]

# Replace biologically implausible values (example: "+" or "-")
numeric_columns = df.select_dtypes(include=["float64", "int64"]).columns
for col in numeric_columns:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Fill missing values using domain-specific formulas
df['RBC'] = df['RBC'].fillna(df['HCT'] * 100 / df['MCV'])
df['HGB'] = df['HGB'].fillna(df['MCHC'] * df['HCT'] / 100)

# Drop irrelevant or noisy columns
df.drop(columns=['Potassium', 'RDW', 'TSH'], inplace=True)

# Step 3: Feature Engineering
# Standardize numerical variables
scaler = StandardScaler()
numerical_features = ['ALT', 'AST', 'HGB', 'PLT', 'MPV', 'EOS%']
df[numerical_features] = scaler.fit_transform(df[numerical_features])

# Encode categorical variables (e.g., ICD-10 codes)
icd10_codes = df['ICD10'].unique()
encoder = LabelEncoder()
df['ICD10_encoded'] = encoder.fit_transform(df['ICD10'])

# Step 4: Feature Selection
# Drop highly correlated features
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm")
plt.title("Feature Correlation Matrix")
plt.show()

# Remove AST (correlated with ALT)
df.drop(columns=['AST'], inplace=True)

# Step 5: Train-Test Split
X = df[['ALT', 'HGB', 'PLT', 'MPV', 'EOS%', 'ICD10_encoded']]
y = df['CancerLabel']  # Binary cancer label (1 for cancer, 0 for non-cancer)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Model Training
# Logistic Regression
log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)
y_pred_log = log_reg.predict(X_test)
log_auc = roc_auc_score(y_test, log_reg.predict_proba(X_test)[:, 1])
print("Logistic Regression AUC:", log_auc)

# Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
rf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])
print("Random Forest AUC:", rf_auc)

# XGBoost
xgb = XGBClassifier(n_estimators=200, max_depth=5, learning_rate=0.1, random_state=42)
xgb.fit(X_train, y_train)
y_pred_xgb = xgb.predict(X_test)
xgb_auc = roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1])
print("XGBoost AUC:", xgb_auc)

# Step 7: Model Evaluation
print("\nLogistic Regression Report:")
print(classification_report(y_test, y_pred_log))

print("\nRandom Forest Report:")
print(classification_report(y_test, y_pred_rf))

print("\nXGBoost Report:")
print(classification_report(y_test, y_pred_xgb))

# Step 8: Feature Importance (Random Forest Example)
feature_importances = rf.feature_importances_
plt.barh(X.columns, feature_importances)
plt.title("Feature Importance")
plt.xlabel("Importance")
plt.ylabel("Feature")
plt.show()

# Step 9: Save Preprocessed Data and Models
df.to_csv("data/preprocessed_data.csv", index=False)
import joblib
joblib.dump(log_reg, "models/logistic_regression.pkl")
joblib.dump(rf, "models/random_forest.pkl")
joblib.dump(xgb, "models/xgboost.pkl")

print("Data preprocessing and model training completed successfully!")
